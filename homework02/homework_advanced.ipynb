{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqtXrZApBkM4"
   },
   "source": [
    "# Optimizing training and inference\n",
    "\n",
    "In this notebook, we will discuss different ways to reduce memory and compute usage during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEt8wg4JCQdm"
   },
   "source": [
    "## Prepare training script\n",
    "\n",
    "When training large models, it is usually a best practice not to use Jupyter notebooks, but run a **separate script** for training which could have command-line flags for various hyperparameters and training modes. This is especially useful when you need to run multiple experiments simultaneously (e.g. on a cluster with task scheduler). Another advantage of this is that after training, the process will finish and free the resources for other users of a shared GPU.\n",
    "\n",
    "**Task**. In this part, you will need to put all your code to train a model on CIFAR-10 that you wrote for the previous task in `train.py`. Your code has to run for at least one epoch (a single run over the whole dataset).\n",
    "\n",
    "You can then run your script from inside of this notebook like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-TWiKq8H9yT"
   },
   "outputs": [],
   "source": [
    "!python3 train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKPYZ3QLEqX8"
   },
   "source": [
    "## Profiling time\n",
    "\n",
    "For the next tasks, you need to add measurements to your training loop. You can use [`perf_counter`](https://docs.python.org/3/library/time.html#time.perf_counter) for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSr-PyQNFkSC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HMJMCGRKFYCc",
    "outputId": "571046a2-443b-465f-ce62-ddaf68b105d0"
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(1000, 1000)\n",
    "y = np.random.randn(1000, 1000)\n",
    "\n",
    "start_counter = time.perf_counter()\n",
    "z = x @ y\n",
    "elapsed_time = time.perf_counter() - start_counter\n",
    "print(f\"Matrix multiplication took {elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfhLeWjTGTpB"
   },
   "source": [
    "**Task**. You need to add the following measurements to your training script:\n",
    "* How much time a forward-backward pass takes for a single batch;\n",
    "* How much time an epoch takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khDOTn_SHaND"
   },
   "source": [
    "## Profiling memory usage\n",
    "\n",
    "**Task**. You need to measure the memory consumptions\n",
    "\n",
    "This section depends on whether you train on CPU or GPU.\n",
    "\n",
    "### If you train on CPU\n",
    "You can use GNU time to measure peak RAM usage of a script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98xvXSjUIDzl"
   },
   "outputs": [],
   "source": [
    "!/usr/bin/time -lp python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1ES2Pc9IlH5"
   },
   "source": [
    "**Maximum resident set size**  will show you the peak RAM usage in bytes after the script finishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kq5lY5CKJHX1"
   },
   "source": [
    "### If you train on GPU\n",
    "\n",
    "Use [`torch.cuda.max_memory_allocated()`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_allocated) at the end of your script to show the maximum amount of memory in bytes used by all tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fSQdauqLIkf1",
    "outputId": "8bcffc30-637d-461a-8f44-0e444a28caae"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 1000, 1000, device='cuda:0')\n",
    "print(f\"Peak memory usage by Pytorch tensors: {(torch.cuda.max_memory_allocated() / 1024 / 1024):.2f} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3RWHxYKBUys"
   },
   "source": [
    "## Gradient based techniques\n",
    "\n",
    "Modern architectures can potentially consume lots and lots of memory even for minibatch of several objects. To handle such cases here we will discuss two simple techniques.\n",
    "\n",
    "### Gradient Checkpointing\n",
    "\n",
    "Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.\n",
    "\n",
    "See [blogpost](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) for kind introduction and different strategies or [article](https://arxiv.org/pdf/1604.06174.pdf) for not kind introduction.\n",
    "\n",
    "**Task**. Use [built-in checkpointing](https://pytorch.org/docs/stable/checkpoint.html), measure the difference in memory/compute \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjY8LR_GQbTV"
   },
   "source": [
    "### Accumulating gradient for large batches\n",
    "We can increase the effective batch size by simply accumulating gradients over multiple forward passes. Note that `loss.backward()` simply adds the computed gradient to `tensor.grad`, so we can call this method multiple times before actually taking an optimizer step. However, this approach might be a little tricky to combine with batch normalization. Do you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbbbO7V0QeGT"
   },
   "outputs": [],
   "source": [
    "effective_batch_size = 1024\n",
    "loader_batch_size = 32\n",
    "batches_per_update = effective_batch_size / loader_batch_size # Updating weights after 8 forward passes\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=loader_batch_size)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for batch_i, (batch_X, batch_y) in enumerate(dataloader):\n",
    "    l = loss(model(batch_X), batch_y)\n",
    "    l.backward() # Adds gradients\n",
    "  \n",
    "    if (batch_i + 1) % batches_per_update == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqxvZWH9Uxtq"
   },
   "source": [
    "**Task**. Explore the trade-off between computation time and memory usage while maintaining the same effective batch size. By effective batch size we mean the number of objects over which the loss is computed before taking a gradient step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3iiJZuhSUR0"
   },
   "source": [
    "## Accuracy vs compute trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WOWhqMJSboR"
   },
   "source": [
    "### Tensor type size\n",
    "\n",
    "One of the hyperparameter affecting memory consumption is the precision (e.g. floating point number). The most popular choices are 32 bit and 64 bit precision. \n",
    "\n",
    "**Task**. Let's turn to your code and analyze how the change of the precision will affect the perfomance and memory. Here two cases:\n",
    "* if you have FloatTensor -> try DoubleTensor. Does the increased precision affect the perfomance?\n",
    "* if you have DoubleTensor -> try FloatTensor. Did you lose perfomance? (in fact, you can even gain!)\n",
    "\n",
    "**Note**.  Training with 16 bit precision is called mixed precision training. 16 bit arithmetics is not implemented by default. Several hacks (https://arxiv.org/pdf/1710.03740.pdf) allow it to work giving you memory gain for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xAEF9aJc-43"
   },
   "source": [
    "### Quantization\n",
    "\n",
    "We can actually move further and use even lower precision like 8-bit integers:\n",
    "\n",
    "* https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd\n",
    "* https://nervanasystems.github.io/distiller/quantization/\n",
    "* https://arxiv.org/abs/1712.05877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXad1svpSk8f"
   },
   "source": [
    "### Knowledge distillation\n",
    "Suppose that we have a large network (*teacher network*) or an ensemble of networks which has a good accuracy. We can like train a much smaller network (*student network*) using the outputs of teacher networks. It turns out that the perfomance could be even better! This approach doesn't help with training speed, but can be quite beneficial when we'd like to reduce the model size for low-memory devices.\n",
    "\n",
    "* https://www.ttic.edu/dl/dark14.pdf\n",
    "* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
    "* https://medium.com/neural-machines/knowledge-distillation-dc241d7c2322\n",
    "\n",
    "**Task**. Distill your (teacher) network with smaller one (student), compare it perfomance with the teacher network and with the same (student) trained directly from data\n",
    "\n",
    "**Note**. You can even use a completely different ([article](https://arxiv.org/abs/1711.10433)) architecture in a student model, e.g. approximate an autoregressive model (WaveNet) by a non-autoregressive one.\n",
    "\n",
    "This approach doesn't help with training speed, but can be quite beneficial when we'd like to reduce the model size for low-memory devices."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework_optimization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
